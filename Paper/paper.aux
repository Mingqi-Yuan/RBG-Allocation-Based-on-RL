\relax 
\citation{Sutton1998Reinforcement}
\@writefile{toc}{\contentsline {section}{\numberline {1}Framework of Reinforcement Learning}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Interaction environment}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{State}{1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Attributes of state}}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Reward}{1}\protected@file@percent }
\citation{Lillicrap2015Continuous}
\citation{HaarnojaSoft}
\citation{long2015fully}
\citation{He2016Deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Agent}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Deep Deterministic Policy Gradient}{2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces DDPG for RBG Allocation}}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}RBGNet}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Backbone}{2}\protected@file@percent }
\citation{Lin2013Network}
\citation{HowardUniversal}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Architecture of RBGNet.The parameters of downsampling layers were configured skillfully based on $ dim_{feature} $ }}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Remark}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Training strategy}{3}\protected@file@percent }
\bibdata{ref}
\bibcite{Sutton1998Reinforcement}{1}
\bibcite{Lillicrap2015Continuous}{2}
\bibcite{HaarnojaSoft}{3}
\bibcite{long2015fully}{4}
\bibcite{He2016Deep}{5}
\bibcite{Lin2013Network}{6}
\bibcite{HowardUniversal}{7}
\bibstyle{ieeetr}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Training strategy}}{4}\protected@file@percent }
